{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1nSLjg-SMuIkGZ9tjfZ7kPCmDxTrD8u-q?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open in Colab\"/></a>"
      ],
      "metadata": {
        "id": "lfzxkoFg4ePS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting Google Drive to Access Dataset"
      ],
      "metadata": {
        "id": "rAhpqH6d3j81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgsT6mC3tQF-"
      },
      "outputs": [],
      "source": [
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBozFjkZgUG2"
      },
      "outputs": [],
      "source": [
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20b1c6af"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "This section imports necessary libraries required for the entire notebook. It includes deep learning libraries such as PyTorch, data manipulation libraries like NumPy, and visualization libraries such as matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fyYNJJ-wXVL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import WeightedRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ea2095"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "This section defines the transformations to be applied to the input data for training and evaluation purposes. It includes normalization, resizing, and augmentation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phwkI5VowXcW"
      },
      "outputs": [],
      "source": [
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oGdFrDawXeu"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "       transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c021840"
      },
      "source": [
        "## Data Directory Setup\n",
        "\n",
        "This sets the path to the directory where the dataset is stored. It's essential for the notebook to access the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TxSDbhXwXhD",
        "outputId": "fb1647ef-5a2b-4b4c-fe9b-b78ba99ff07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['benign', 'malignant']\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/drive/MyDrive/Ensemble Learning on LIDC Dataset/data\"  # Set the directory for the data\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in [ 'test', 'train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val','test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['test', 'train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "num_classes = len(class_names)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97b79c30"
      },
      "source": [
        "## Visualization Function\n",
        "\n",
        "Here we define a function to visualize images in the dataset. It will help in understanding the data and debugging the data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t957VPeRwXjx"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3t4wxmKwXmK"
      },
      "outputs": [],
      "source": [
        "# Get a batch of testing data\n",
        "inputs, classes = next(iter(dataloaders['test']))\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCNW2ZBbwXod"
      },
      "outputs": [],
      "source": [
        "def plot(val_loss,train_loss,typ):\n",
        "    plt.title(\"{} after epoch: {}\".format(typ,len(train_loss)))\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(typ)\n",
        "    plt.plot(list(range(len(train_loss))),train_loss,color=\"r\",label=\"Train \"+typ)\n",
        "    plt.plot(list(range(len(val_loss))),val_loss,color=\"b\",label=\"Validation \"+typ)\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(data_dir,typ+\".png\"))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ4sbUwCwXq_"
      },
      "outputs": [],
      "source": [
        "val_loss_gph=[]\n",
        "train_loss_gph=[]\n",
        "val_acc_gph=[]\n",
        "train_acc_gph=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc576835"
      },
      "source": [
        "## Model Training Function\n",
        "\n",
        "This function encapsulates the model training logic. It takes a model, criterion for loss calculation, optimizer for backpropagation, and a scheduler for learning rate adjustment as inputs and conducts the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MUg_T9FwXty"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25,model_name = \"kaggle\"):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) #was (outputs,1) for non-inception and (outputs.data,1) for inception\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "              train_loss_gph.append(epoch_loss)\n",
        "              train_acc_gph.append(epoch_acc)\n",
        "            if phase == 'val':\n",
        "              val_loss_gph.append(epoch_loss)\n",
        "              val_acc_gph.append(epoch_acc)\n",
        "           # plot(val_loss_gph,train_loss_gph, \"Loss\")\n",
        "          #  plot(val_acc_gph,train_acc_gph, \"Accuracy\")\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc >= best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model, data_dir+\"/\"+model_name+\".h5\")\n",
        "                print('==>Model Saved')\n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet 152"
      ],
      "metadata": {
        "id": "7wZ0_Qfo5f8q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9302949"
      },
      "source": [
        "## Model Definition and Training\n",
        "\n",
        "This section covers the instantiation of the ResNet152 model and its subsequent training with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSghfYK5wXwK"
      },
      "outputs": [],
      "source": [
        "model = models.resnet152(pretrained = True)\n",
        "#num_ftrs = model.classifier[0].in_features\n",
        "num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n",
        "#num_ftrs = model.classifier.in_features  ## for densenet169\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"resnet152\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGn8_Nw1wXy6"
      },
      "outputs": [],
      "source": [
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "import csv\n",
        "import numpy as np  # Importing NumPy for numerical operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc-8RBcowX1r"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/resnet152_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1DjIkdPwX4O"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/resnet152_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Sz8Gm3K7oO"
      },
      "source": [
        "# Inception V3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb03cdc6"
      },
      "source": [
        "## Inception V3 Model\n",
        "\n",
        "Following the pattern of the previous section, this part focuses on the Inception V3 model, its setup, and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs1MXn4oL4oO"
      },
      "outputs": [],
      "source": [
        "model = models.inception_v3(pretrained = True)\n",
        "model.aux_logits = False\n",
        "# Handle the auxilary net\n",
        "num_ftrs = model.AuxLogits.fc.in_features\n",
        "model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "# Handle the primary net\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs,num_classes)\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"inception_v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmh_rbxbMVQy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()\n",
        "correct = 0\n",
        "total = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQeUPZyMYY0"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/inception_v3_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV1bWRWrMZRD"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/inception_v3_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Densenet 169"
      ],
      "metadata": {
        "id": "xpFGud4Y5mz3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeHrwVuZTZ4c"
      },
      "source": [
        "\n",
        "\n",
        "## Densenet 169  Model\n",
        "\n",
        "Following the pattern of the previous section, this part focuses on the Densenet 169 model, its setup, and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xgJj2xGTarw"
      },
      "outputs": [],
      "source": [
        "model = models.densenet169(pretrained = True)\n",
        "#num_ftrs = model.classifier[0].in_features\n",
        "#num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n",
        "num_ftrs = model.classifier.in_features  ## for densenet169\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "#model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"densenet169\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqimg_IxUQjm"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()\n",
        "correct = 0\n",
        "total = 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D79oEFEVUM5j"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/densenet169_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIkkc94UUNog"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/densenet169_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficientnet B7 Model"
      ],
      "metadata": {
        "id": "w2BmxFk25qt_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-_BIBsS-Qcw"
      },
      "source": [
        "## Efficientnet B7 Model\n",
        "\n",
        "Following the pattern of the previous section, this part focuses on the Efficientnet B7 model, its setup, and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjFMd7jC-Q4W"
      },
      "outputs": [],
      "source": [
        "model = models.efficientnet_b7(pretrained = True)\n",
        "#num_ftrs = model.classifier[0].in_features\n",
        "#num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n",
        "#num_ftrs = model.classifier.in_features  ## for densenet169\n",
        "num_ftrs = model.classifier[1].in_features   ## for efficientnet_b7\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "#model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169, efficientnet_b7\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"efficientnet_b7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhT4agZ7-Q7n"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()\n",
        "correct = 0\n",
        "total = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x6tAo7e_tWB"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/efficientnetb7_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "411CiJvX_tYo"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/efficientnetb7_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}