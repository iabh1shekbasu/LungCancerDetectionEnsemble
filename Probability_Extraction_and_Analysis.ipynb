{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/iabh1shekbasu/LungCancerDetectionEnsemble/blob/main/Probability_Extraction_and_Analysis.ipynb\n",
        ")\n"
      ],
      "metadata": {
        "id": "lfzxkoFg4ePS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Connecting Google Drive"
      ],
      "metadata": {
        "id": "rAhpqH6d3j81"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dgsT6mC3tQF-"
      },
      "outputs": [],
      "source": [
        "!google-drive-ocamlfuse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBozFjkZgUG2"
      },
      "outputs": [],
      "source": [
        "!xdg-settings set default-web-browser w3m.desktop # to set default browser\n",
        "%cd /content\n",
        "!mkdir drive\n",
        "%cd drive\n",
        "!mkdir MyDrive\n",
        "%cd ..\n",
        "%cd ..\n",
        "!google-drive-ocamlfuse /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20b1c6af"
      },
      "source": [
        "## Importing Libraries\n",
        "\n",
        "This section imports necessary libraries required for the entire notebook. It includes deep learning libraries such as PyTorch, data manipulation libraries like NumPy, and visualization libraries such as matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fyYNJJ-wXVL"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import os\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "from torch.utils.data import WeightedRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2ea2095"
      },
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "This section defines the transformations to be applied to the input data for training and evaluation purposes. It includes normalization, resizing, and augmentation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "phwkI5VowXcW"
      },
      "outputs": [],
      "source": [
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9oGdFrDawXeu"
      },
      "outputs": [],
      "source": [
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "    ]),\n",
        "    'val': transforms.Compose([\n",
        "        transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "       transforms.Resize((256,256)),\n",
        "        transforms.ToTensor(),\n",
        "    ]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c021840"
      },
      "source": [
        "## Data Directory Setup\n",
        "\n",
        "This sets the path to the directory where the dataset is stored. It's essential for the notebook to access the training and testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TxSDbhXwXhD",
        "outputId": "fb1647ef-5a2b-4b4c-fe9b-b78ba99ff07c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['benign', 'malignant']\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"/content/drive/MyDrive/Ensemble Learning on LIDC Dataset/data\"  # Set the directory for the data\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in [ 'test', 'train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n",
        "                                             shuffle=True, num_workers=2)\n",
        "              for x in ['train', 'val','test']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['test', 'train', 'val']}\n",
        "class_names = image_datasets['train'].classes\n",
        "num_classes = len(class_names)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97b79c30"
      },
      "source": [
        "## Visualization Function\n",
        "\n",
        "Here we define a function to visualize images in the dataset. It will help in understanding the data and debugging the data loaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t957VPeRwXjx"
      },
      "outputs": [],
      "source": [
        "def imshow(inp, title):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3t4wxmKwXmK"
      },
      "outputs": [],
      "source": [
        "# Get a batch of testing data\n",
        "inputs, classes = next(iter(dataloaders['test']))\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCNW2ZBbwXod"
      },
      "outputs": [],
      "source": [
        "def plot(val_loss,train_loss,typ):\n",
        "    plt.title(\"{} after epoch: {}\".format(typ,len(train_loss)))\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(typ)\n",
        "    plt.plot(list(range(len(train_loss))),train_loss,color=\"r\",label=\"Train \"+typ)\n",
        "    plt.plot(list(range(len(val_loss))),val_loss,color=\"b\",label=\"Validation \"+typ)\n",
        "    plt.legend()\n",
        "    plt.savefig(os.path.join(data_dir,typ+\".png\"))\n",
        "    plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ4sbUwCwXq_"
      },
      "outputs": [],
      "source": [
        "val_loss_gph=[]\n",
        "train_loss_gph=[]\n",
        "val_acc_gph=[]\n",
        "train_acc_gph=[]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc576835"
      },
      "source": [
        "## Model Training Function\n",
        "\n",
        "This function encapsulates the model training logic. It takes a model, criterion for loss calculation, optimizer for backpropagation, and a scheduler for learning rate adjustment as inputs and conducts the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MUg_T9FwXty"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25,model_name = \"kaggle\"):\n",
        "    since = time.time()\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch+1, num_epochs))\n",
        "        print('-' * 10)\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1) #was (outputs,1) for non-inception and (outputs.data,1) for inception\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        optimizer.zero_grad()\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "            if phase == 'train':\n",
        "              train_loss_gph.append(epoch_loss)\n",
        "              train_acc_gph.append(epoch_acc)\n",
        "            if phase == 'val':\n",
        "              val_loss_gph.append(epoch_loss)\n",
        "              val_acc_gph.append(epoch_acc)\n",
        "           # plot(val_loss_gph,train_loss_gph, \"Loss\")\n",
        "          #  plot(val_acc_gph,train_acc_gph, \"Accuracy\")\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc >= best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "                torch.save(model, data_dir+\"/\"+model_name+\".h5\")\n",
        "                print('==>Model Saved')\n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet 152"
      ],
      "metadata": {
        "id": "7wZ0_Qfo5f8q"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9302949"
      },
      "source": [
        "## Model Definition and Training\n",
        "\n",
        "This section covers the instantiation of the ResNet152 model and its subsequent training with the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSghfYK5wXwK"
      },
      "outputs": [],
      "source": [
        "model = models.resnet152(pretrained = True)\n",
        "#num_ftrs = model.classifier[0].in_features\n",
        "num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n",
        "#num_ftrs = model.classifier.in_features  ## for densenet169\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"resnet152\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGn8_Nw1wXy6"
      },
      "outputs": [],
      "source": [
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()\n",
        "import csv\n",
        "import numpy as np  # Importing NumPy for numerical operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vc-8RBcowX1r"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/resnet152_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n1DjIkdPwX4O"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/resnet152_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E5Sz8Gm3K7oO"
      },
      "source": [
        "# Inception V3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb03cdc6"
      },
      "source": [
        "## Inception V3 Model\n",
        "\n",
        "Following the pattern of the previous section, this part focuses on the Inception V3 model, its setup, and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qs1MXn4oL4oO"
      },
      "outputs": [],
      "source": [
        "model = models.inception_v3(pretrained = True)\n",
        "model.aux_logits = False\n",
        "# Handle the auxilary net\n",
        "num_ftrs = model.AuxLogits.fc.in_features\n",
        "model.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n",
        "# Handle the primary net\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs,num_classes)\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"inception_v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmh_rbxbMVQy"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyQeUPZyMYY0"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/inception_v3_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV1bWRWrMZRD"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/inception_v3_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Densenet 169"
      ],
      "metadata": {
        "id": "xpFGud4Y5mz3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IeHrwVuZTZ4c"
      },
      "source": [
        "\n",
        "\n",
        "## Densenet 169  Model\n",
        "\n",
        "Following the pattern of the previous section, this part focuses on the Densenet 169 model, its setup, and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xgJj2xGTarw"
      },
      "outputs": [],
      "source": [
        "model = models.densenet169(pretrained = True)\n",
        "#num_ftrs = model.classifier[0].in_features\n",
        "#num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n",
        "num_ftrs = model.classifier.in_features  ## for densenet169\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "#model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"densenet169\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqimg_IxUQjm"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "\n",
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D79oEFEVUM5j"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/densenet169_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIkkc94UUNog"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/densenet169_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Efficientnet B7 Model"
      ],
      "metadata": {
        "id": "w2BmxFk25qt_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-_BIBsS-Qcw"
      },
      "source": [
        "## Efficientnet B7 Model\n",
        "\n",
        "Following the pattern of the previous section, this part focuses on the Efficientnet B7 model, its setup, and training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjFMd7jC-Q4W"
      },
      "outputs": [],
      "source": [
        "model = models.efficientnet_b7(pretrained = True)\n",
        "#num_ftrs = model.classifier[0].in_features\n",
        "#num_ftrs = model.fc.in_features  ##for googlenet, resnet18\n",
        "#num_ftrs = model.classifier.in_features  ## for densenet169\n",
        "num_ftrs = model.classifier[1].in_features   ## for efficientnet_b7\n",
        "print(\"Number of features: \"+str(num_ftrs))\n",
        "#model.classifier = nn.Linear(num_ftrs, num_classes) ## for vgg19\n",
        "#model.fc = nn.Linear(num_ftrs, num_classes)  ##for googlenet, resnet18\n",
        "model.classifier = nn.Linear(num_ftrs, num_classes) ## for densenet169, efficientnet_b7\n",
        "model = model.to(device)\n",
        "criterion = nn.CrossEntropyLoss( weight = torch.tensor([1, 4.7]).to(device))\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "# StepLR Decays the learning rate of each parameter group by gamma every step_size epochs\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "# Learning rate scheduling should be applied after optimizer’s update\n",
        "# e.g., you should write your code this way:\n",
        "# for epoch in range(100):\n",
        "#     train(...)\n",
        "#     validate(...)\n",
        "#     scheduler.step()\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma=0.1)\n",
        "model = train_model(model, criterion, optimizer, step_lr_scheduler, num_epochs=10, model_name = \"efficientnet_b7\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhT4agZ7-Q7n"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "# Getting Proba distribution\n",
        "print(\"\\nGetting the Probability Distribution\")\n",
        "trainloader=torch.utils.data.DataLoader(image_datasets['train'],batch_size=1)\n",
        "testloader=torch.utils.data.DataLoader(image_datasets['test'],batch_size=1)\n",
        "model=model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_x6tAo7e_tWB"
      },
      "outputs": [],
      "source": [
        "f = open(data_dir+\"/efficientnetb7_train.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(trainloader),num_classes))\n",
        "      for i,data in enumerate(trainloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = trainloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Train Accuracy = \",100*correct/total)\n",
        "for i in range(len(trainloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/train_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(trainloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = trainloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "411CiJvX_tYo"
      },
      "outputs": [],
      "source": [
        "#Test Probabilities\n",
        "f = open(data_dir+\"/efficientnetb7_test.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "saving = []\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "      num = 0\n",
        "      temp_array = np.zeros((len(testloader),num_classes))\n",
        "      for i,data in enumerate(testloader):\n",
        "          images, labels = data\n",
        "          sample_fname, _ = testloader.dataset.samples[i]\n",
        "          labels=labels.cuda()\n",
        "          outputs = model(images.cuda())\n",
        "          _, predicted = torch.max(outputs, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels.cuda()).sum().item()\n",
        "          prob = torch.nn.functional.softmax(outputs, dim=1)\n",
        "          saving.append(sample_fname.split('/')[-1])\n",
        "          temp_array[num] = np.asarray(prob[0].tolist()[0:num_classes])\n",
        "          num+=1\n",
        "print(\"Test Accuracy = \",100*correct/total)\n",
        "for i in range(len(testloader)):\n",
        "  k = temp_array[i].tolist()\n",
        "  k.append(saving[i])\n",
        "  writer.writerow(k)\n",
        "f.close()\n",
        "f = open(data_dir+\"/test_labels.csv\",'w+',newline = '')\n",
        "writer = csv.writer(f)\n",
        "for i,data in enumerate(testloader):\n",
        "  _, labels = data\n",
        "  sample_fname, _ = testloader.dataset.samples[i]\n",
        "  sample = sample_fname.split('/')[-1]\n",
        "  lab = labels.tolist()[0]\n",
        "  writer.writerow([sample,lab])\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning Analysis"
      ],
      "metadata": {
        "id": "T_wzB7pcMV7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "F60vUT2KM9mh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import *\n",
        "import matplotlib.pyplot as plt\n",
        "import math,os,argparse\n",
        "from scikitplot.estimators import plot_feature_importances\n",
        "from scikitplot.metrics import plot_confusion_matrix, plot_roc\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize"
      ],
      "metadata": {
        "id": "1yFTPabIM6cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "_WIM3Au8NBPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser.add_argument('--root_train', type=str, required = True, help='Directory where train csv files are stored')\n",
        "parser.add_argument('--train_labels', type=str, required = True, help='File path for train labels')\n",
        "parser.add_argument('--root_test', type=str, required = True, help='Directory where test csv files are stored')\n",
        "parser.add_argument('--test_labels', type=str, required = True, help='File path for test labels')\n",
        "    df = pd.read_csv(file,header=None)\n",
        "    df = pd.read_csv(file,header=None)"
      ],
      "metadata": {
        "id": "lgJezXwONBi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Helper Functions"
      ],
      "metadata": {
        "id": "CxHDVc1qNFtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predicting(ensemble_prob):\n",
        "    prediction = np.zeros((ensemble_prob.shape[0],))\n",
        "    for i in range(ensemble_prob.shape[0]):\n",
        "        temp = ensemble_prob[i]\n",
        "        t = np.where(temp == np.max(temp))[0][0]\n",
        "        prediction[i] = t\n",
        "    return prediction\n",
        "\n",
        "def getfile(filename):\n",
        "    root=\"./\"\n",
        "    file = root+filename\n",
        "    if '.csv' not in file:\n",
        "        file+='.csv'\n",
        "    df = pd.read_csv(file,header=None)\n",
        "    df = np.asarray(df)[:,:-1] #Since last column has image names\n",
        "    return df\n",
        "\n",
        "def metrics(labels,predictions,classes):\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(labels, predictions, target_names = classes,digits = 4))\n",
        "    matrix = confusion_matrix(labels, predictions)\n",
        "    accuracy = accuracy_score(labels,predictions)\n",
        "    pre = precision_score(labels,predictions)\n",
        "    rec = recall_score(labels,predictions)\n",
        "    f1 = f1_score(labels,predictions)\n",
        "    auc = roc_auc_score(labels,predictions)\n",
        "    print(\"Accuracy\", accuracy)\n",
        "    print(\"Precision Score\", pre)\n",
        "    print(\"Recall Score\", rec)\n",
        "    print(\"F1 Score\", f1)\n",
        "    print(\"Roc_Auc Score\", auc)\n",
        "    print(\"Confusion matrix:\")\n",
        "    print(matrix)\n",
        "    print(\"\\nClasswise Accuracy :{}\".format(matrix.diagonal()/matrix.sum(axis = 1)))"
      ],
      "metadata": {
        "id": "Vviab_TxNDuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot ROC Curve Function"
      ],
      "metadata": {
        "id": "E7QrH7bjNMIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ROC-AUC\n",
        "from sklearn.metrics import roc_curve,auc\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "def plot_roc(val_label,decision_val, caption='ROC Curve'):\n",
        "    num_classes=np.unique(val_label).shape[0]\n",
        "    classes = []\n",
        "    for i in range(num_classes):\n",
        "        classes.append(i)\n",
        "    plt.figure()\n",
        "    decision_val = label_binarize(decision_val, classes=classes)\n",
        "\n",
        "    if num_classes!=2:\n",
        "        # Compute ROC curve and ROC area for each class\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        for i in range(num_classes):\n",
        "            y_val = label_binarize(val_label, classes=classes)\n",
        "            fpr[i], tpr[i], _ = roc_curve(y_val[:, i], decision_val[:, i])\n",
        "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "        for i in range(num_classes):\n",
        "            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n",
        "                                           ''.format(i+1, roc_auc[i]))\n",
        "    else:\n",
        "        fpr,tpr,_ = roc_curve(val_label,decision_val, pos_label=1)\n",
        "        roc_auc = auc(fpr,tpr)*100\n",
        "        plt.plot(fpr,tpr,label='ROC curve (AUC=%0.2f)'%roc_auc)\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(caption)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig(str(len(classes))+'RE_NAME.png',dpi=300)\n",
        "\n",
        "\n",
        "#plot_roc(test_labels,preds_Adjusted)\n",
        "plot_roc(test_labels,preds_original)"
      ],
      "metadata": {
        "id": "7jQLF1IaNSKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis\n"
      ],
      "metadata": {
        "id": "zUooQ98FNsVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate scores for each model\n",
        "def get_scores(labels, *models_predictions):\n",
        "    \"\"\"\n",
        "    Calculates precision, recall, f1-score, and AUC for each model's predictions.\n",
        "\n",
        "    :param labels: The ground truth labels.\n",
        "    :param models_predictions: Variable number of arrays with model predictions.\n",
        "    :return: A list of weights for each set of model predictions.\n",
        "    \"\"\"\n",
        "    num_models = len(models_predictions)\n",
        "    # Initialize metrics array\n",
        "    metrics = np.zeros((2, num_models))\n",
        "    num_classes = len(np.unique(labels))\n",
        "\n",
        "    for i, model_preds in enumerate(models_predictions):\n",
        "        # Simulate a predicting function for model predictions\n",
        "        preds = predicting(model_preds)\n",
        "\n",
        "        # Calculate different metrics depending on the number of classes\n",
        "        if num_classes == 2:  # Binary classification\n",
        "            pre = precision_score(labels, preds)\n",
        "            rec = recall_score(labels, preds)\n",
        "            f1 = f1_score(labels, preds)\n",
        "            auc = roc_auc_score(labels, preds)\n",
        "        else:  # Multiclass classification\n",
        "            pre = precision_score(labels, preds, average='macro')\n",
        "            rec = recall_score(labels, preds, average='macro')\n",
        "            f1 = f1_score(labels, preds, average='macro')\n",
        "            auc = roc_auc_score(labels, model_preds, average='macro', multi_class='ovo')\n",
        "\n",
        "        # Update metrics array with the calculated metrics\n",
        "        metrics[:, i] = np.array([f1, auc])\n",
        "\n",
        "    # Output the f1 and auc scores for each model\n",
        "    print(\"F1 Score\", metrics[0] * 100)\n",
        "    print(\"ROC_AUC Score\", metrics[1] * 100)\n",
        "\n",
        "    # Calculate weights based on the metrics\n",
        "    weights = get_weights(np.transpose(metrics))\n",
        "    return weights\n",
        "\n",
        "def get_weights(matrix):\n",
        "    \"\"\"\n",
        "    Calculates weights for each model using the tanh function on the metrics.\n",
        "\n",
        "    :param matrix: A matrix of shape (number of models, number of metrics).\n",
        "    :return: A list of weights for each model.\n",
        "    \"\"\"\n",
        "    weights = []\n",
        "    for model_metrics in matrix:\n",
        "        # Use tanh to compute a weighted sum of metrics for each model\n",
        "        weight = np.sum(np.tanh(model_metrics))\n",
        "        weights.append(weight)\n",
        "    return weights\n",
        "\n",
        "# Ensure the training root path ends with a slash\n",
        "root_train = args.root_train\n",
        "if root_train[-1] != '/':\n",
        "    root_train += '/'\n",
        "\n",
        "# Ensure the testing root path ends with a slash\n",
        "root_test = args.root_test\n",
        "if root_test[-1] != '/':\n",
        "    root_test += '/'\n",
        "\n",
        "# Define the filenames for the training predictions from different models\n",
        "train1 = \"densenet169_train_adam\"\n",
        "train2 = \"efficientnetb7_train_adam\"\n",
        "train3 = \"resnet152_train_adam\"\n",
        "\n",
        "# Retrieve the predictions from files for training data\n",
        "p1_train = getfile(root_train + train1)  # Get predictions for the first model\n",
        "p2_train = getfile(root_train + train2)  # Get predictions for the second model\n",
        "p3_train = getfile(root_train + train3)  # Get predictions for the third model\n",
        "\n",
        "# Load the labels for the training data\n",
        "train_labels = getlabels(args.train_labels)\n",
        "\n",
        "# Define the filenames for the testing predictions from different models\n",
        "test1 = \"densenet169_test_adam\"\n",
        "test2 = \"efficientnetb7_test_adam\"\n",
        "test3 = \"resnet152_test_adam\"\n",
        "\n",
        "# Retrieve the predictions from files for testing data\n",
        "p1_test = getfile(root_test + test1)  # Get predictions for the first model\n",
        "p2_test = getfile(root_test + test2)  # Get predictions for the second model\n",
        "p3_test = getfile(root_test + test3)  # Get predictions for the third model\n",
        "\n",
        "# Load the labels for the testing data\n",
        "test_labels = getlabels(args.test_labels)\n",
        "\n",
        "# Print the order of CSV files\n",
        "print(\"Train CSV's Order\", train1 + \", \" + train2 + \", \" + train3)\n",
        "print(\"\\n\")\n",
        "print(\"Test CSV's Order\", test1 + \", \" + test2 + \", \" + test3)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display header for training data metrics\n",
        "print(\"Training Data Metrics\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(train1 + \" \" + train2 + \" \" + train3)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "\n",
        "# Get weights based on training labels and predictions\n",
        "weights = get_scores(train_labels, p1_train, p2_train, p3_train)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display header for testing data metrics\n",
        "print(\"Testing Data Metrics\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(test1 + \" \" + test2 + \" \" + test3)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "weight = get_scores(test_labels, p1_test, p2_test, p3_test)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Display original weights\n",
        "print(\"Original Weights\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(train1, weights[0])\n",
        "print(train2, weights[1])\n",
        "print(train3, weights[2])\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculate ensemble probabilities using the original weights\n",
        "ensemble_prob_original = weights[0] * p1_test + weights[1] * p2_test + weights[2] * p3_test\n",
        "\n",
        "# Display ensemble probabilities with original weights\n",
        "print(\"Ensemble Probabilities with Original Weights\", ensemble_prob_original)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Generate predictions using the original ensemble probabilities\n",
        "preds_original = predicting(ensemble_prob_original)\n",
        "\n",
        "# Display predictions with original weights\n",
        "print(\"Predictions with Original Weights\", preds_original)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Normalize the weights so they sum up to 1\n",
        "w0, w1, w2 = weights\n",
        "w0_norm = w0 / (w0 + w1 + w2)\n",
        "w1_norm = w1 / (w0 + w1 + w2)\n",
        "w2_norm = w2 / (w0 + w1 + w2)\n",
        "\n",
        "# Display normalized weights\n",
        "print(\"Normalised Weights\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(train1, w0_norm)\n",
        "print(train2, w1_norm)\n",
        "print(train3, w2_norm)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculate ensemble probabilities using the normalized weights\n",
        "ensemble_prob_Normal = w0_norm * p1_test + w1_norm * p2_test + w2_norm * p3_test\n",
        "\n",
        "# Display ensemble probabilities with normalized weights\n",
        "print(\"Ensemble Probabilities with Normal Weights\", ensemble_prob_Normal)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Generate predictions using the normalized ensemble probabilities\n",
        "preds_Normal = predicting(ensemble_prob_Normal)\n",
        "\n",
        "# Display predictions with normalized weights\n",
        "print(\"Predictions with Normal Weights\", preds_Normal)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Manually adjusted weights based on empirical findings\n",
        "w0_adj = 0.3585864 # Best Weights for some criterion\n",
        "w1_adj = 0.3441670 # Best Weights for some criterion\n",
        "w2_adj = 0.2972466 # Best Weights for some criterion\n",
        "\n",
        "# Display adjusted weights\n",
        "print(\"Adjusted Weights\")\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(train1, w0_adj)\n",
        "print(train2, w1_adj)\n",
        "print(train3, w2_adj)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculate ensemble probabilities using the adjusted weights\n",
        "ensemble_prob_Adjusted = w0_adj * p1_test + w1_adj * p2_test + w2_adj * p3_test\n",
        "\n",
        "# Display ensemble probabilities after updating weights\n",
        "print(\"Ensemble Probabilities After Updating Weights\", ensemble_prob_Adjusted)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Generate predictions using the adjusted ensemble probabilities\n",
        "preds_Adjusted = predicting(ensemble_prob_Adjusted)\n",
        "\n",
        "# Display predictions after updating weights\n",
        "print(\"Predictions After Updating Weights\", preds_Adjusted)\n",
        "print(\"--------------------------------------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "# Calculate the number of correct predictions with original weights and print accuracy\n",
        "correct_original = np.where(preds_original == test_labels)[0].shape[0]\n",
        "total = test_labels.shape[0]\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy (Original) = \", (correct_original / total) * 100)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print classification metrics for the predictions made with original weights\n",
        "classes = ['Benign', 'Malignant']\n",
        "metrics(test_labels, preds_original, classes)\n",
        "\n",
        "# Calculate the number of correct predictions with normalized weights and print accuracy\n",
        "correct_Normal = np.where(preds_Normal == test_labels)[0].shape[0]\n",
        "print(\"\\n\")\n",
        "print(\"Accuracy (Normal) = \", (correct_Normal / total) * 100)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Print classification metrics for the predictions made with normalized weights\n",
        "metrics(test_labels, preds_Normal, classes)"
      ],
      "metadata": {
        "id": "jlgPiVbaNu63"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}